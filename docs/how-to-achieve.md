# 项目实现说明

## Transformer模型架构

本项目实现了基于"Attention Is All You Need"论文的Transformer模型。Transformer是一种基于注意力机制的序列到序列模型，主要由编码器和解码器两部分组成。

### 核心组件

1. **多头注意力机制**：允许模型关注输入序列的不同位置，并行计算多个注意力头，然后合并结果。

2. **前馈神经网络**：每个编码器和解码器层后面跟随一个简单的前馈网络。

3. **位置编码**：为序列中的每个位置添加位置信息，使模型能够理解序列的顺序。

4. **残差连接和层归一化**：每个子层周围都有残差连接，并跟随层归一化操作。

### 数学公式

自注意力机制的核心公式：

```
Attention(Q, K, V) = softmax(QK^T/√d_k)V
```

其中：
- Q是查询矩阵
- K是键矩阵
- V是值矩阵
- d_k是键向量的维度

多头注意力机制：

```
MultiHead(Q, K, V) = Concat(head_1, ..., head_h)W^O
```

其中：
```
head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)
```

## 项目结构

项目采用模块化设计，主要包含以下组件：

1. **模型层**：实现了Transformer的基本层，如多头注意力、前馈网络等。

2. **Transformer模型**：组合基本层构建完整的Transformer模型。

3. **数据加载**：负责加载和预处理训练数据。

4. **评估指标**：实现了BLEU等评估指标。

5. **实用工具**：提供了模型保存/加载、配置管理等功能。

6. **训练脚本**：实现了完整的训练流程。

7. **推理脚本**：用于使用训练好的模型进行预测。

8. **部署脚本**：将模型部署为API服务。

## 训练过程

训练过程包括：

1. 加载和预处理数据
2. 初始化模型、优化器和损失函数
3. 执行训练循环，包括前向传播、损失计算、反向传播和参数更新
4. 定期评估模型性能并保存最佳模型

训练过程中使用了学习率调度器和梯度裁剪来提高训练稳定性。

## 推理和部署

推理过程使用贪婪解码策略生成预测结果。部署时，使用Flask框架将模型包装为API服务，提供了预测和健康检查两个端点。

这个项目支持CPU和GPU计算，默认使用CPU，当检测到GPU时会自动切换。